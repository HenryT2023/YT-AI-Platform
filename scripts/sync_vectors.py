#!/usr/bin/env python3
"""
å‘é‡ç´¢å¼•å…¨é‡åŒæ­¥è„šæœ¬

ç”¨æ³•:
    python scripts/sync_vectors.py --tenant-id yantian
    python scripts/sync_vectors.py --tenant-id yantian --site-id yantian-main
    python scripts/sync_vectors.py --tenant-id yantian --dry-run

åŠŸèƒ½:
    1. ä» PostgreSQL è¯»å–æ‰€æœ‰ evidence
    2. å‘é‡åŒ–å¹¶å†™å…¥ Qdrant
    3. æ›´æ–° evidence.vector_updated_at
    4. è®°å½•åŒæ­¥ä»»åŠ¡åˆ° vector_sync_jobs
"""

import argparse
import asyncio
import hashlib
import sys
import time
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional
from uuid import uuid4

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, "/Users/hal/YT-AI-Platform/services/core-backend")

import structlog
from sqlalchemy import func, select, update
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker

logger = structlog.get_logger(__name__)


# ============================================================
# é…ç½®
# ============================================================

DATABASE_URL = "postgresql+asyncpg://yantian:yantian@localhost:5432/yantian"
QDRANT_URL = "http://localhost:6333"
QDRANT_COLLECTION = "yantian_evidence"
BATCH_SIZE = 50
VECTOR_DIM = 1024


# ============================================================
# Embedding è·å–
# ============================================================

async def get_embedding(text: str, settings: Dict[str, str]) -> Optional[List[float]]:
    """è·å–æ–‡æœ¬å‘é‡"""
    import httpx

    # ä¼˜å…ˆä½¿ç”¨ OpenAI
    openai_key = settings.get("OPENAI_API_KEY", "")
    if openai_key:
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                resp = await client.post(
                    "https://api.openai.com/v1/embeddings",
                    headers={
                        "Authorization": f"Bearer {openai_key}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "model": "text-embedding-3-small",
                        "input": text[:8000],
                    },
                )
                if resp.status_code == 200:
                    data = resp.json()
                    return data["data"][0]["embedding"]
        except Exception as e:
            logger.error("openai_embedding_error", error=str(e))

    # å›é€€ï¼šBaidu
    baidu_key = settings.get("BAIDU_API_KEY", "")
    baidu_secret = settings.get("BAIDU_SECRET_KEY", "")
    if baidu_key and baidu_secret:
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                token_resp = await client.post(
                    "https://aip.baidubce.com/oauth/2.0/token",
                    params={
                        "grant_type": "client_credentials",
                        "client_id": baidu_key,
                        "client_secret": baidu_secret,
                    },
                )
                if token_resp.status_code != 200:
                    return None
                access_token = token_resp.json().get("access_token")

                embed_resp = await client.post(
                    f"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/embeddings/embedding-v1?access_token={access_token}",
                    json={"input": [text[:1000]]},
                )
                if embed_resp.status_code == 200:
                    data = embed_resp.json()
                    if "data" in data and len(data["data"]) > 0:
                        return data["data"][0]["embedding"]
        except Exception as e:
            logger.error("baidu_embedding_error", error=str(e))

    return None


def compute_content_hash(title: Optional[str], excerpt: str) -> str:
    """è®¡ç®—å†…å®¹ hash"""
    content = f"{title or ''}\n{excerpt}"
    return hashlib.sha256(content.encode()).hexdigest()


def generate_point_id(evidence_id: str) -> str:
    """ç”Ÿæˆ Qdrant point ID"""
    hash_bytes = hashlib.md5(evidence_id.encode()).hexdigest()
    return f"{hash_bytes[:8]}-{hash_bytes[8:12]}-{hash_bytes[12:16]}-{hash_bytes[16:20]}-{hash_bytes[20:32]}"


# ============================================================
# åŒæ­¥é€»è¾‘
# ============================================================

async def sync_vectors(
    tenant_id: str,
    site_id: Optional[str] = None,
    dry_run: bool = False,
    batch_size: int = BATCH_SIZE,
    settings: Optional[Dict[str, str]] = None,
) -> Dict[str, Any]:
    """
    å…¨é‡åŒæ­¥å‘é‡

    Args:
        tenant_id: ç§Ÿæˆ· ID
        site_id: ç«™ç‚¹ IDï¼ˆå¯é€‰ï¼‰
        dry_run: åªç»Ÿè®¡ï¼Œä¸å†™å…¥
        batch_size: æ‰¹æ¬¡å¤§å°
        settings: é…ç½®ï¼ˆåŒ…å« API keysï¼‰

    Returns:
        åŒæ­¥ç»“æœç»Ÿè®¡
    """
    from app.database.models import Evidence, VectorSyncJob

    settings = settings or {}
    start_time = time.time()
    job_id = str(uuid4())

    logger.info(
        "sync_vectors_start",
        job_id=job_id,
        tenant_id=tenant_id,
        site_id=site_id,
        dry_run=dry_run,
    )

    # åˆ›å»ºæ•°æ®åº“è¿æ¥
    engine = create_async_engine(DATABASE_URL, echo=False)
    async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

    # ç»Ÿè®¡
    stats = {
        "job_id": job_id,
        "tenant_id": tenant_id,
        "site_id": site_id,
        "dry_run": dry_run,
        "total": 0,
        "success": 0,
        "skip": 0,
        "failure": 0,
        "errors": [],
    }

    async with async_session() as session:
        # 1. åˆ›å»ºåŒæ­¥ä»»åŠ¡è®°å½•
        if not dry_run:
            job = VectorSyncJob(
                id=job_id,
                tenant_id=tenant_id,
                site_id=site_id,
                job_type="full_sync",
                status="running",
                started_at=datetime.now(timezone.utc),
                config={"batch_size": batch_size, "dry_run": dry_run},
                triggered_by="cli",
            )
            session.add(job)
            await session.commit()

        # 2. æŸ¥è¯¢ evidence æ€»æ•°
        count_stmt = select(func.count(Evidence.id)).where(
            Evidence.tenant_id == tenant_id,
            Evidence.deleted_at.is_(None),
        )
        if site_id:
            count_stmt = count_stmt.where(Evidence.site_id == site_id)

        result = await session.execute(count_stmt)
        total_count = result.scalar() or 0
        stats["total"] = total_count

        logger.info("sync_vectors_total", total=total_count)

        if total_count == 0:
            logger.info("sync_vectors_no_data")
            return stats

        # 3. åˆå§‹åŒ– Qdrant
        if not dry_run:
            from qdrant_client import QdrantClient
            from qdrant_client.models import VectorParams, Distance

            qdrant = QdrantClient(url=QDRANT_URL, timeout=30)

            # ç¡®ä¿ collection å­˜åœ¨
            collections = qdrant.get_collections().collections
            collection_names = [c.name for c in collections]
            if QDRANT_COLLECTION not in collection_names:
                qdrant.create_collection(
                    collection_name=QDRANT_COLLECTION,
                    vectors_config=VectorParams(
                        size=VECTOR_DIM,
                        distance=Distance.COSINE,
                    ),
                )
                logger.info("qdrant_collection_created", name=QDRANT_COLLECTION)

        # 4. åˆ†æ‰¹å¤„ç†
        total_batches = (total_count + batch_size - 1) // batch_size
        current_batch = 0

        offset = 0
        while offset < total_count:
            current_batch += 1
            logger.info(
                "sync_vectors_batch",
                batch=current_batch,
                total_batches=total_batches,
                offset=offset,
            )

            # æŸ¥è¯¢ä¸€æ‰¹ evidence
            stmt = (
                select(Evidence)
                .where(
                    Evidence.tenant_id == tenant_id,
                    Evidence.deleted_at.is_(None),
                )
                .order_by(Evidence.created_at)
                .offset(offset)
                .limit(batch_size)
            )
            if site_id:
                stmt = stmt.where(Evidence.site_id == site_id)

            result = await session.execute(stmt)
            evidences = result.scalars().all()

            if not evidences:
                break

            # å¤„ç†æ¯æ¡ evidence
            points_to_upsert = []
            evidence_ids_to_update = []

            for evidence in evidences:
                try:
                    # è®¡ç®—å†…å®¹ hash
                    content_hash = compute_content_hash(evidence.title, evidence.excerpt)

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                    if evidence.vector_hash == content_hash:
                        stats["skip"] += 1
                        continue

                    if dry_run:
                        stats["success"] += 1
                        continue

                    # æ„å»ºæ–‡æœ¬
                    text_parts = []
                    if evidence.title:
                        text_parts.append(evidence.title)
                    text_parts.append(evidence.excerpt)
                    text = "\n".join(text_parts)

                    # è·å–å‘é‡
                    embedding = await get_embedding(text, settings)
                    if not embedding:
                        stats["failure"] += 1
                        stats["errors"].append({
                            "evidence_id": evidence.id,
                            "error": "no_embedding",
                        })
                        continue

                    # æ„å»º point
                    from qdrant_client.models import PointStruct

                    point_id = generate_point_id(evidence.id)
                    points_to_upsert.append(
                        PointStruct(
                            id=point_id,
                            vector=embedding,
                            payload={
                                "evidence_id": evidence.id,
                                "tenant_id": evidence.tenant_id,
                                "site_id": evidence.site_id,
                                "source_type": evidence.source_type,
                                "source_ref": evidence.source_ref,
                                "title": evidence.title,
                                "excerpt": evidence.excerpt[:500],
                                "confidence": evidence.confidence,
                                "verified": evidence.verified,
                                "tags": evidence.tags or [],
                                "domains": evidence.domains or [],
                            },
                        )
                    )
                    evidence_ids_to_update.append((evidence.id, content_hash))
                    stats["success"] += 1

                except Exception as e:
                    stats["failure"] += 1
                    stats["errors"].append({
                        "evidence_id": evidence.id,
                        "error": str(e),
                    })
                    logger.error("sync_vectors_evidence_error", evidence_id=evidence.id, error=str(e))

            # æ‰¹é‡å†™å…¥ Qdrant
            if points_to_upsert and not dry_run:
                qdrant.upsert(
                    collection_name=QDRANT_COLLECTION,
                    points=points_to_upsert,
                )
                logger.info("qdrant_upsert", count=len(points_to_upsert))

            # æ‰¹é‡æ›´æ–° evidence.vector_updated_at
            if evidence_ids_to_update and not dry_run:
                now = datetime.now(timezone.utc)
                for eid, ehash in evidence_ids_to_update:
                    await session.execute(
                        update(Evidence)
                        .where(Evidence.id == eid)
                        .values(vector_updated_at=now, vector_hash=ehash)
                    )
                await session.commit()

            # æ›´æ–°ä»»åŠ¡è¿›åº¦
            if not dry_run:
                progress = min(100.0, (offset + len(evidences)) / total_count * 100)
                await session.execute(
                    update(VectorSyncJob)
                    .where(VectorSyncJob.id == job_id)
                    .values(
                        progress_percent=progress,
                        current_batch=current_batch,
                        total_batches=total_batches,
                        success_count=stats["success"],
                        skip_count=stats["skip"],
                        failure_count=stats["failure"],
                    )
                )
                await session.commit()

            offset += batch_size

        # 5. å®Œæˆä»»åŠ¡
        elapsed = time.time() - start_time
        stats["elapsed_seconds"] = elapsed

        if not dry_run:
            status = "success" if stats["failure"] == 0 else "partial_failed"
            await session.execute(
                update(VectorSyncJob)
                .where(VectorSyncJob.id == job_id)
                .values(
                    status=status,
                    finished_at=datetime.now(timezone.utc),
                    total_items=stats["total"],
                    success_count=stats["success"],
                    skip_count=stats["skip"],
                    failure_count=stats["failure"],
                    progress_percent=100.0,
                    error_summary={"errors": stats["errors"][:100]},  # åªä¿ç•™å‰ 100 æ¡é”™è¯¯
                )
            )
            await session.commit()

        logger.info(
            "sync_vectors_complete",
            job_id=job_id,
            total=stats["total"],
            success=stats["success"],
            skip=stats["skip"],
            failure=stats["failure"],
            elapsed_seconds=elapsed,
        )

    await engine.dispose()
    return stats


# ============================================================
# CLI
# ============================================================

def print_stats(stats: Dict[str, Any]) -> None:
    """æ‰“å°ç»Ÿè®¡ç»“æœ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š å‘é‡åŒæ­¥ç»“æœ")
    print("=" * 60)
    print(f"  Job ID:       {stats['job_id']}")
    print(f"  Tenant:       {stats['tenant_id']}")
    print(f"  Site:         {stats.get('site_id', 'all')}")
    print(f"  Dry Run:      {stats['dry_run']}")
    print("-" * 60)
    print(f"  æ€» Evidence:  {stats['total']}")
    print(f"  æˆåŠŸå‘é‡åŒ–:   {stats['success']}")
    print(f"  è·³è¿‡(é‡å¤):   {stats['skip']}")
    print(f"  å¤±è´¥:         {stats['failure']}")
    print("-" * 60)

    if stats['total'] > 0:
        coverage = (stats['success'] + stats['skip']) / stats['total'] * 100
        print(f"  è¦†ç›–ç‡:       {coverage:.1f}%")

    if 'elapsed_seconds' in stats:
        print(f"  è€—æ—¶:         {stats['elapsed_seconds']:.2f}s")

    if stats['errors']:
        print("-" * 60)
        print(f"  é”™è¯¯è¯¦æƒ… (å‰ 5 æ¡):")
        for err in stats['errors'][:5]:
            print(f"    - {err['evidence_id']}: {err['error']}")

    print("=" * 60)


async def main():
    parser = argparse.ArgumentParser(description="å‘é‡ç´¢å¼•å…¨é‡åŒæ­¥")
    parser.add_argument("--tenant-id", required=True, help="ç§Ÿæˆ· ID")
    parser.add_argument("--site-id", help="ç«™ç‚¹ IDï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--dry-run", action="store_true", help="åªç»Ÿè®¡ï¼Œä¸å†™å…¥")
    parser.add_argument("--batch-size", type=int, default=BATCH_SIZE, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--openai-key", help="OpenAI API Key")
    parser.add_argument("--baidu-key", help="Baidu API Key")
    parser.add_argument("--baidu-secret", help="Baidu Secret Key")

    args = parser.parse_args()

    # æ„å»ºé…ç½®
    import os
    settings = {
        "OPENAI_API_KEY": args.openai_key or os.environ.get("OPENAI_API_KEY", ""),
        "BAIDU_API_KEY": args.baidu_key or os.environ.get("BAIDU_API_KEY", ""),
        "BAIDU_SECRET_KEY": args.baidu_secret or os.environ.get("BAIDU_SECRET_KEY", ""),
    }

    print(f"\nğŸš€ å¼€å§‹å‘é‡åŒæ­¥...")
    print(f"   Tenant: {args.tenant_id}")
    print(f"   Site:   {args.site_id or 'all'}")
    print(f"   Mode:   {'DRY RUN' if args.dry_run else 'LIVE'}")

    stats = await sync_vectors(
        tenant_id=args.tenant_id,
        site_id=args.site_id,
        dry_run=args.dry_run,
        batch_size=args.batch_size,
        settings=settings,
    )

    print_stats(stats)

    # è¿”å›ç 
    if stats["failure"] > 0:
        sys.exit(1)
    sys.exit(0)


if __name__ == "__main__":
    asyncio.run(main())
